{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ad34fd-4e15-4277-b05b-c0bde4404137",
   "metadata": {},
   "source": [
    "# Часть 3. Who's the mightiest of them all?\n",
    "## Постройте 4 варианта градиентного бустинга, используя значения гиперпараметров “из коробки”: реализация из sklearn, XGBoost, CatBoost, LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "615d62a6-ef74-460d-a688-b9bea4831bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier(random_state=1)\n",
      "F1 score на тестовой выборке\t: 0.563\n",
      "AUC-ROC на тестовой выборке\t: 0.700\n",
      "\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=1, ...)\n",
      "F1 score на тестовой выборке\t: 0.540\n",
      "AUC-ROC на тестовой выборке\t: 0.687\n",
      "\n",
      "<catboost.core.CatBoostClassifier object at 0x0000028664B34DD0>\n",
      "F1 score на тестовой выборке\t: 0.578\n",
      "AUC-ROC на тестовой выборке\t: 0.709\n",
      "\n",
      "LGBMClassifier(random_state=1, verbose=-1)\n",
      "F1 score на тестовой выборке\t: 0.571\n",
      "AUC-ROC на тестовой выборке\t: 0.706\n"
     ]
    }
   ],
   "source": [
    "# ! pip install lightgbm\n",
    "# ! pip install catboost\n",
    "# pip install xgboost\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier \n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "df_train = pd.read_csv(r'.\\2.Split dataset to train-test\\train.csv')\n",
    "df_test = pd.read_csv(r'.\\2.Split dataset to train-test\\test.csv')\n",
    "\n",
    "X_train, y_train = df_train.drop('Churn', axis=1), df_train['Churn']\n",
    "X_test, y_test = df_test.drop('Churn', axis=1), df_test['Churn']\n",
    "\n",
    "#print(X_train.head())\n",
    "#print(y_train.head())\n",
    "#print(X_test.head())\n",
    "#print(y_test.head())\n",
    "\n",
    "def simple_test(model):\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    #y_pred = model.predict(X_train)\n",
    "    #print('F1 score на тренировочной выборке\\t: {:.3f}'.format(f1_score(y_train, y_pred)))\n",
    "    #print(\"AUC-ROC на тренировочной выборке\\t: {:.3f}\".format(roc_auc_score(y_train, y_pred)))\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('F1 score на тестовой выборке\\t: {:.3f}'.format(f1_score(y_test, y_pred)))\n",
    "    print(\"AUC-ROC на тестовой выборке\\t: {:.3f}\".format(roc_auc_score(y_test, y_pred)))\n",
    "\n",
    "\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "simple_test(model)\n",
    "#print(model.get_params())\n",
    "print()\n",
    "\n",
    "model = XGBClassifier(random_state=1)\n",
    "simple_test(model)\n",
    "#print(model.get_xgb_params())\n",
    "print()\n",
    "\n",
    "model =  CatBoostClassifier(random_state=1, logging_level=\"Silent\")\n",
    "simple_test(model)\n",
    "#print(model.get_all_params())\n",
    "print()\n",
    "\n",
    "model = LGBMClassifier(random_state=1, verbose=-1)\n",
    "simple_test(model)\n",
    "#print(model.get_params())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744166ab-c396-489c-ad94-d0b990042fe7",
   "metadata": {},
   "source": [
    "### Кто лидирует\n",
    "\n",
    "Лучше всего метрики f1, AUC-ROC у CatBoostClassifier.\n",
    "\n",
    "## Настройка гиперпараметров моделей на кросс-валидации\n",
    "Подберем набор гиперпараметров по кросс-валидационной оценке по тренировочной выборке.\n",
    "После этого прогоним финишный тест на отложенной тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38081c9a-d9e4-495b-ac5e-e09492bf2b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подбор гиперпараметров: GradientBoostingClassifier(random_state=1)\n",
      "f1  на кросс-валидации \t: 0.590\n",
      "Best parameters:  {'learning_rate': 0.1, 'loss': 'exponential', 'max_depth': 2, 'max_features': 'sqrt', 'n_estimators': 100, 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# ! pip install lightgbm\n",
    "# ! pip install catboost\n",
    "# pip install xgboost\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier \n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "df_train = pd.read_csv(r'.\\2.Split dataset to train-test\\train.csv')\n",
    "df_test = pd.read_csv(r'.\\2.Split dataset to train-test\\test.csv')\n",
    "\n",
    "X_train, y_train = df_train.drop('Churn', axis=1), df_train['Churn']\n",
    "X_test, y_test = df_test.drop('Churn', axis=1), df_test['Churn']\n",
    "\n",
    "scoring = 'f1'\n",
    "\n",
    "#print(X_train.head())\n",
    "#print(y_train.head())\n",
    "#print(X_test.head())\n",
    "#print(y_test.head())\n",
    "\n",
    "def print_finish_metrics(model, clf):\n",
    "    print(model)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print('F1 score на финишной тестовой выборке \\t: {:.3f}'.format(f1_score(y_test, y_pred)))\n",
    "    print(\"AUC-ROC на финишной тестовой выборке \\t: {:.3f}\".format(roc_auc_score(y_test, y_pred)))\n",
    "    print()\n",
    "\n",
    "def RunGridSearchCV(model, parameters, is_finish = False):\n",
    "    print('Подбор гиперпараметров:', model)\n",
    "    clf = GridSearchCV(model, parameters, scoring = scoring, cv = 5, refit=True, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(scoring, ' на кросс-валидации \\t: {:.3f}'.format(clf.best_score_))\n",
    "    print('Best parameters: ', clf.best_params_)\n",
    "    return clf\n",
    "    \n",
    "\n",
    "parameters = {'learning_rate': [0.1, 0.01, 0.001],  \n",
    "              'n_estimators':[10, 100, 200],\n",
    "              'max_depth': [1, 2],\n",
    "              'max_features':[None, 'sqrt'],\n",
    "              'subsample': [0.5, 1.0],\n",
    "              'loss': ['log_loss', 'exponential']\n",
    "              }\n",
    "model = GradientBoostingClassifier(random_state=1)\n",
    "clf_GradientBoostingClassifier = RunGridSearchCV(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "895105ec-9202-4a56-b36a-3a9224706957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подбор гиперпараметров: XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=1, ...)\n",
      "f1  на кросс-валидации \t: 0.588\n",
      "Best parameters:  {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 200, 'objective': 'binary:logistic', 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'learning_rate': [0.1, 0.01, 0.001],  \n",
    "              'n_estimators':[10, 100, 200],\n",
    "              'max_depth':[1, 2],\n",
    "              'subsample': [0.5, 1.0],\n",
    "              'objective': ['binary:logistic', 'binary:hinge']\n",
    "              }\n",
    "model = XGBClassifier(random_state=1)\n",
    "clf_XGBClassifier = RunGridSearchCV(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fccb19d4-eb12-460b-aab6-ee2638d9dded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подбор гиперпараметров: <catboost.core.CatBoostClassifier object at 0x00000286720EFA50>\n",
      "f1  на кросс-валидации \t: 0.589\n",
      "Best parameters:  {'depth': 4, 'iterations': 100, 'learning_rate': 0.1, 'loss_function': 'Logloss', 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'learning_rate': [0.1, 0.01, 0.001],  \n",
    "              'iterations':[10, 50, 100],\n",
    "              'subsample': [0.5, 1.0],\n",
    "              'depth': [1, 2, 4],\n",
    "              'loss_function': ['Logloss', 'CrossEntropy']\n",
    "              }\n",
    "model =  CatBoostClassifier(random_state=1, logging_level=\"Silent\", thread_count=8)\n",
    "clf_CatBoostClassifier = RunGridSearchCV(model, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12d37f28-377a-4c9b-8e3d-ca539297d65e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подбор гиперпараметров: LGBMClassifier(metric='f1', n_jobs=-1, random_state=1, verbose=-1)\n",
      "f1  на кросс-валидации \t: 0.576\n",
      "Best parameters:  {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 40, 'objective': 'binary', 'subsample': 0.5}\n"
     ]
    }
   ],
   "source": [
    "parameters = {'learning_rate': [0.1, 0.075, 0.01, 0.001], \n",
    "              'n_estimators':[5, 10, 25, 30, 40],\n",
    "              'subsample': [0.5, 0.75, 1.0],\n",
    "              'max_depth': [1, 2, 3],\n",
    "              'objective': ['binary', 'cross_entropy', None]\n",
    "              }\n",
    "model = LGBMClassifier(random_state=1, metric='f1', verbose=-1, n_jobs=-1)\n",
    "clf_LGBMClassifier = RunGridSearchCV(model, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12745d39-e009-4d0d-876d-f50855dbf3d4",
   "metadata": {},
   "source": [
    "### Финишный тест моделей\n",
    "Прогоним тест на финишной отложенной тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b18c2f9f-d66c-4035-8065-640e0ad9edfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier\n",
      "F1 score на финишной тестовой выборке \t: 0.562\n",
      "AUC-ROC на финишной тестовой выборке \t: 0.700\n",
      "\n",
      "XGBClassifier\n",
      "F1 score на финишной тестовой выборке \t: 0.561\n",
      "AUC-ROC на финишной тестовой выборке \t: 0.699\n",
      "\n",
      "CatBoostClassifier\n",
      "F1 score на финишной тестовой выборке \t: 0.568\n",
      "AUC-ROC на финишной тестовой выборке \t: 0.703\n",
      "\n",
      "LGBMClassifier\n",
      "F1 score на финишной тестовой выборке \t: 0.544\n",
      "AUC-ROC на финишной тестовой выборке \t: 0.688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_finish_metrics(\"GradientBoostingClassifier\", clf_GradientBoostingClassifier)\n",
    "print_finish_metrics(\"XGBClassifier\", clf_XGBClassifier)\n",
    "print_finish_metrics(\"CatBoostClassifier\", clf_CatBoostClassifier)\n",
    "print_finish_metrics(\"LGBMClassifier\", clf_LGBMClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efece7c-30d0-4bba-bee4-2b863e2af05f",
   "metadata": {},
   "source": [
    "### Кто лидирует\n",
    "\n",
    "Лучше всего метрики f1, AUC-ROC у CatBoostClassifier.\n",
    "Но абсолютный победитель: CatBoostClassifier \"из коробки\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fae7f9-c84e-4981-aac1-3259144b0343",
   "metadata": {},
   "source": [
    "## Эксперимент: логистическая регрессия\n",
    "На этапе EDA был применен Метод Внедрения с логистической регрессией. Наблюдение показало, что логистическая регрессия хорошо обучилась по набору данных.\n",
    "Поэтому проверим результаты с логистической регрессией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a1650dd-102e-45cc-953f-f631a60b38cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score на тестовой выборке\t: 0.569\n",
      "AUC-ROC на тестовой выборке: 0.704\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "df_train = pd.read_csv(r'.\\2.Split dataset to train-test\\train.csv')\n",
    "df_test = pd.read_csv(r'.\\2.Split dataset to train-test\\test.csv')\n",
    "\n",
    "X_train, y_train = df_train.drop('Churn', axis=1), df_train['Churn']\n",
    "X_test, y_test = df_test.drop('Churn', axis=1), df_test['Churn']\n",
    "\n",
    "model_log_lr1 = LogisticRegression(C=1, solver='liblinear', penalty='l2', random_state=1)\n",
    "model_log_lr1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model_log_lr1.predict(X_test)\n",
    "\n",
    "print('F1 score на тестовой выборке\\t: {:.3f}'.format(f1_score(y_test, y_pred)))\n",
    "print(\"AUC-ROC на тестовой выборке: {:.3f}\".format(roc_auc_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b37f30-3a59-443f-99ec-fdb03571dbcd",
   "metadata": {},
   "source": [
    "*Вывод по эксперименту: логистическая регрессия на данном наборе данных показала себя на уровне \"бустингов\".*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d24c575-d25a-46a6-8ce3-bc789a8d3696",
   "metadata": {},
   "source": [
    "## Эксперимент 2: Bagging из \"бустингов\"\n",
    "\"Бустинги\" дают низкий Bias, а Bagging уменьшает Variance. Могут ли они дать лучше результаты вместе: низкий Bias и Variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85e0dbd7-5c4d-4b04-8df5-7f32ea461b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score на тестовой выборке\t: 0.570\n",
      "AUC-ROC на тестовой выборке\t: 0.704\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier \n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "df_train = pd.read_csv(r'.\\2.Split dataset to train-test\\train.csv')\n",
    "df_test = pd.read_csv(r'.\\2.Split dataset to train-test\\test.csv')\n",
    "\n",
    "X_train, y_train = df_train.drop('Churn', axis=1), df_train['Churn']\n",
    "X_test, y_test = df_test.drop('Churn', axis=1), df_test['Churn']\n",
    "\n",
    "model = BaggingClassifier(estimator=LGBMClassifier(random_state=1, max_depth=7, n_estimators=100, verbose=-1), \n",
    "                          n_jobs=-1, \n",
    "                          n_estimators=100,\n",
    "                          random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print('F1 score на тестовой выборке\\t: {:.3f}'.format(f1_score(y_test, y_pred)))\n",
    "print(\"AUC-ROC на тестовой выборке\\t: {:.3f}\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf28631-a71d-40e7-898a-0f65c738ec7a",
   "metadata": {},
   "source": [
    "*Вывод по эксперименту: качество не поднялось.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
