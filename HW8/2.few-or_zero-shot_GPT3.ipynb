{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# few-/zero-shot c GPT3\n",
    "\n",
    "Скачаем и используем [ai-forever/rugpt3large_based_on_gpt2](https://huggingface.co/ai-forever/rugpt3large_based_on_gpt2/tree/main)\n",
    "\n",
    "Идея классификации: используем loss модели для определения правильности построения фраз. Если фраза построена правильно, то у GPT не должно возникнуть затруднений в работе. Если фраза неправильная, то loss должен получиться выше.\n",
    "\n",
    "Обучим гиперпараметр threshold (уровень разделения классов) по тренировочному набору данных, после этого проверим результаты на тестовом наборе данных.\n",
    "\n",
    "Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef\n",
    "from torch.utils.data import TensorDataset\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'H:\\Инструменты\\Windows\\GPT or another LLM\\ai-forever rugpt3large_based_on_gpt2 2020/'\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Необходимые определения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_train_loss_threshold(model, tokenizer, df_train, n):\n",
    "    positive_loss = []\n",
    "    negative_loss = []\n",
    "\n",
    "    for i in tqdm(range(min(len(df_train), n))):\n",
    "        loss = inference(model, tokenizer, df_train.iloc[i].sentence)\n",
    "\n",
    "        if df_train.iloc[i].acceptable == 1:\n",
    "            positive_loss.append(loss)\n",
    "        else:\n",
    "            negative_loss.append(loss)\n",
    "\n",
    "    # Здесь можно улучшить, если ввести доп.гиперпараметр, чтобы задавать: к какому классу ближе порог.\n",
    "    # Сейчас усреднение делается ровно по среднему между объектами обоих классов.\n",
    "    return (sum(negative_loss) / len(negative_loss) + sum(positive_loss) / len(positive_loss)) / 2.\n",
    "\n",
    "\n",
    "def inference(model, tokenizer, phrase):\n",
    "    with torch.no_grad():\n",
    "        encodings = tokenizer(phrase, return_tensors='pt')\n",
    "        input_ids = encodings.input_ids\n",
    "        outputs = model(input_ids=input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def test(model, tokenizer, threshold, phrase, verbose = True):\n",
    "    if verbose:\n",
    "        print(phrase)\n",
    "\n",
    "    loss = inference(model, tokenizer, phrase)\n",
    "\n",
    "    if loss < threshold:\n",
    "      return 1\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "\n",
    "def get_test_predictions(model, tokenizer, threshold, df_test, n):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for i in tqdm(range(min(len(df_test), n))):\n",
    "        acceptable_pred = test(model, tokenizer, threshold, df_test.iloc[i].sentence, verbose=False)\n",
    "        predictions.append(acceptable_pred)\n",
    "        labels.append(df_test.iloc[i].acceptable)\n",
    "\n",
    "    return predictions, labels\n",
    "\n",
    "\n",
    "def get_matthews_corrcoef_score(predictions, true_labels):\n",
    "    # Calculate the MCC\n",
    "    mcc = matthews_corrcoef(true_labels, predictions)\n",
    "    return mcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим наборы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset:\n",
      "                                             sentence  acceptable\n",
      "0                          О староверах я уже писал.           1\n",
      "1  Христофоров, явившийся в ложу первым и одиноко...           0\n",
      "2  Она так и сидела в гостинной, окруженная полто...           0\n",
      "3  Он купит машину, если только не пропьет все де...           1\n",
      "4                           Детям не было где спать.           1\n",
      "Test dataset:\n",
      "                                             sentence  acceptable\n",
      "0                            Иван вчера не позвонил.           1\n",
      "1  У многих туристов, кто посещают Кемер весной, ...           0\n",
      "2  Лесные запахи набегали волнами; в них смешалос...           1\n",
      "3  Вчера президент имел неофициальную беседу с ан...           1\n",
      "4  Коллега так и не признал вину за катастрофу пе...           1\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('in_domain_train_subset.csv')\n",
    "df_test = pd.read_csv('in_domain_test.csv')\n",
    "print('Train dataset:\\n', df_train.head())\n",
    "print('Test dataset:\\n', df_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим tokenizer и модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer...\n",
      "Ok\n"
     ]
    }
   ],
   "source": [
    "print('Loading the tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(PATH)\n",
    "print('Ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучим гиперпараметр threshold на тренировочном наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6924/6924 [13:40<00:00,  8.44it/s] \n"
     ]
    }
   ],
   "source": [
    "threshold = calc_train_loss_threshold(model, tokenizer, df_train, 7000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Грубо проверим результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = tensor(4.0431)\n",
      "Expected:\t 1\n",
      "Лесные запахи набегали волнами; в них смешалось дыхание можжевельника, вереска, брусники.\n",
      "Predict:\t 1\n",
      "\n",
      "Expected:\t 0\n",
      "Вчера в два часа магазин закрыт.\n",
      "Predict:\t 0\n",
      "\n",
      "Expected:\t 0\n",
      "Кракадыл и друсья\n",
      "Predict:\t 0\n",
      "\n",
      "Expected:\t 1\n",
      "Крокодил Гена и его друзья.\n",
      "Predict:\t 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('threshold =', threshold)\n",
    "\n",
    "phrases = [\"Лесные запахи набегали волнами; в них смешалось дыхание можжевельника, вереска, брусники.\",\n",
    "           \"Вчера в два часа магазин закрыт.\",\n",
    "           'Кракадыл и друсья',\n",
    "           'Крокодил Гена и его друзья.'\n",
    "]\n",
    "expecteds = [1, 0, 0, 1]\n",
    "\n",
    "for phrase, expected in zip(phrases, expecteds):\n",
    "    print('Expected:\\t', expected)\n",
    "    print('Predict:\\t', test(model, tokenizer, threshold, phrase))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что, в принципе, получается корректный результат.\n",
    "\n",
    "### Проверим на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 983/983 [01:43<00:00,  9.49it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions, true_labels = get_test_predictions(model, tokenizer, threshold, df_test, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MCC: 0.072\n"
     ]
    }
   ],
   "source": [
    "mcc = get_matthews_corrcoef_score(predictions, true_labels)\n",
    "print('Total MCC: %.3f' % mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- на грубых примерах подход работает хорошо: когда фразы явно неправильные — модель это определяет;\n",
    "- на сложных тонких примерах тестового набора данных результаты получились низкие, на сложном dataset с тонкими дефектами построения фраз лучше результат с Bert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
