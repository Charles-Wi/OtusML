{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RuT5\n",
    "\n",
    "Скачаем и используем [ai-forever/ruT5-base](https://huggingface.co/ai-forever/ruT5-base/tree/main)\n",
    "\n",
    "«…Обучите и протестируйте модель RuT5 на данной задаче (пример finetun’а можете найти здесь https://github.com/RussianNLP/RuCoLA/blob/main/baselines/finetune_t5.py)…»\n",
    "\n",
    "Импорты библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_metric, Dataset, DatasetDict\n",
    "from razdel import tokenize\n",
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим глобальные параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'H:\\Инструменты\\Windows\\GPT or another LLM\\ruT5-base 2021'\n",
    "ACCURACY = load_metric(\"accuracy\", keep_in_memory=True)\n",
    "MCC = load_metric(\"matthews_correlation\", keep_in_memory=True)\n",
    "POS_LABEL = \"грамотно\"\n",
    "NEG_LABEL = \"неграмотно\"\n",
    "\n",
    "CURRENT_DIR = Path('.')\n",
    "DATA_DIR = CURRENT_DIR\n",
    "TRAIN_FILE = DATA_DIR / \"in_domain_train_subset.csv\"\n",
    "IN_DOMAIN_DEV_FILE = DATA_DIR / \"in_domain_validation_subset.csv\"\n",
    "TEST_FILE = DATA_DIR / \"in_domain_test.csv\"\n",
    "\n",
    "N_EPOCHS = 10\n",
    "LR_VALUES = (1e-4,) # 1e-3)\n",
    "DECAY_VALUES = (0,) # 1e-4)\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимые определения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p, tokenizer):\n",
    "    string_preds = tokenizer.batch_decode(p.predictions, skip_special_tokens=True)\n",
    "    int_preds = [1 if prediction == POS_LABEL else 0 for prediction in string_preds]\n",
    "\n",
    "    labels = np.where(p.label_ids != -100, p.label_ids, tokenizer.pad_token_id)\n",
    "    string_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    int_labels = []\n",
    "\n",
    "    for string_label in string_labels:\n",
    "        if string_label == POS_LABEL:\n",
    "            int_labels.append(1)\n",
    "        elif string_label == NEG_LABEL or string_label == \"\":  # second case accounts for test data\n",
    "            int_labels.append(0)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "    acc_result = ACCURACY.compute(predictions=int_preds, references=int_labels)\n",
    "    mcc_result = MCC.compute(predictions=int_preds, references=int_labels)\n",
    "\n",
    "    result = {\"accuracy\": acc_result[\"accuracy\"], \"mcc\": mcc_result[\"matthews_correlation\"]}\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess_data(sentences, tokenizer):\n",
    "    result = tokenizer(sentences[\"sentence\"], padding=False)\n",
    "\n",
    "    if \"acceptable\" in sentences:\n",
    "        label_sequences = []\n",
    "        for label in sentences['acceptable']:\n",
    "            if label == 1:\n",
    "                target_sequence = POS_LABEL\n",
    "            elif label == 0:\n",
    "                target_sequence = NEG_LABEL\n",
    "            else:\n",
    "                raise ValueError(\"Unknown class label\")\n",
    "            label_sequences.append(target_sequence)\n",
    "    else:\n",
    "        # a hack to avoid the \"You have to specify either decoder_input_ids or decoder_inputs_embeds\" error\n",
    "        # for test data\n",
    "        label_sequences = [\"\" for _ in sentences]\n",
    "\n",
    "    result[\"labels\"] = tokenizer(label_sequences, padding=False)[\"input_ids\"]\n",
    "    result[\"length\"] = [len(list(tokenize(sentence))) for sentence in sentences['sentence']]\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_splits(*, as_datasets):\n",
    "    train_df, in_domain_dev_df, test_df = map(\n",
    "        pd.read_csv, (TRAIN_FILE, IN_DOMAIN_DEV_FILE, TEST_FILE)\n",
    "    )\n",
    "\n",
    "    # concatenate datasets to get aggregate metrics\n",
    "    dev_df = in_domain_dev_df #pd.concat((in_domain_dev_df, out_of_domain_dev_df))\n",
    "\n",
    "    if as_datasets:\n",
    "        train, dev, test = map(Dataset.from_pandas, (train_df, dev_df, test_df))\n",
    "        return DatasetDict(train=train, dev=dev, test=test)\n",
    "    else:\n",
    "        return train_df, dev_df, test_df\n",
    "\n",
    "\n",
    "def train(tokenizer, data_collator, tokenized_splits):\n",
    "    # seed, lr, wd, bs\n",
    "    dev_metrics_per_run = np.empty((len(LR_VALUES), len(DECAY_VALUES), 2))\n",
    "\n",
    "    for i, learning_rate in enumerate(LR_VALUES):\n",
    "        for j, weight_decay in enumerate(DECAY_VALUES):\n",
    "            model = T5ForConditionalGeneration.from_pretrained(PATH)\n",
    "\n",
    "            run_base_dir = f\"ruT5_{learning_rate}_{weight_decay}_{BATCH_SIZE}\"\n",
    "\n",
    "            training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=f\"checkpoints/{run_base_dir}\",\n",
    "                overwrite_output_dir=True,\n",
    "                evaluation_strategy=\"epoch\",\n",
    "                per_device_train_batch_size=BATCH_SIZE,\n",
    "                per_device_eval_batch_size=BATCH_SIZE,\n",
    "                learning_rate=learning_rate,\n",
    "                weight_decay=weight_decay,\n",
    "                num_train_epochs=N_EPOCHS,\n",
    "                lr_scheduler_type=\"constant\",\n",
    "                save_strategy=\"epoch\",\n",
    "                save_total_limit=1,\n",
    "                seed=1,\n",
    "                fp16=True,\n",
    "                dataloader_num_workers=4,\n",
    "                group_by_length=True,\n",
    "                report_to=\"none\",\n",
    "                load_best_model_at_end=True,\n",
    "                metric_for_best_model=\"eval_mcc\",\n",
    "                optim=\"adafactor\",\n",
    "                predict_with_generate=True,\n",
    "            )\n",
    "\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=tokenized_splits[\"train\"],\n",
    "                eval_dataset=tokenized_splits[\"dev\"],\n",
    "                compute_metrics=partial(compute_metrics, tokenizer=tokenizer),\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=data_collator,\n",
    "            )\n",
    "\n",
    "            train_result = trainer.train()\n",
    "            print(f\"{run_base_dir}\")\n",
    "            print(\"train\", train_result.metrics)\n",
    "\n",
    "            os.makedirs(f\"results/{run_base_dir}\", exist_ok=True)\n",
    "\n",
    "            dev_predictions = trainer.predict(\n",
    "                test_dataset=tokenized_splits[\"dev\"], metric_key_prefix=\"validation\", max_length=10\n",
    "            )\n",
    "            print(\"dev\", dev_predictions.metrics)\n",
    "            dev_metrics_per_run[i, j] = (\n",
    "                dev_predictions.metrics[\"validation_mcc\"],\n",
    "            )\n",
    "\n",
    "            #rmtree(f\"checkpoints/{run_base_dir}\")\n",
    "    \n",
    "    os.makedirs(\"results_agg\", exist_ok=True)\n",
    "    np.save(f\"results_agg/T5_dev.npy\", dev_metrics_per_run)    \n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"ai-forever/ruT5-base\")\n",
    "#tokenizer.save_pretrained(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузим и подготовим наборы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_data = read_splits(as_datasets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95770ef9bb1042e39af4f153210d1895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabe2ca0bb6b4291978c137431911724",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3185d9e079b84203b0380ea7c8af62b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_splits_data = splits_data.map(\n",
    "    partial(preprocess_data, tokenizer=tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=[\"sentence\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тренировка модели (fine-tune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ea4a97a6eb4c3bb345690a3bbcb45f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b01734773444fe08a148410cc99dd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24565842747688293, 'eval_accuracy': 0.744973544973545, 'eval_mcc': 0.0, 'eval_runtime': 21.8884, 'eval_samples_per_second': 43.174, 'eval_steps_per_second': 0.365, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "287587869ed94e2b89040f0a91dfbdf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22860047221183777, 'eval_accuracy': 0.744973544973545, 'eval_mcc': 0.0, 'eval_runtime': 22.585, 'eval_samples_per_second': 41.842, 'eval_steps_per_second': 0.354, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dec678d5461b44c689ca111b6c0e6fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23818540573120117, 'eval_accuracy': 0.744973544973545, 'eval_mcc': 0.0, 'eval_runtime': 21.9671, 'eval_samples_per_second': 43.019, 'eval_steps_per_second': 0.364, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b07f1e5b2e564e078b92507133d47368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23778873682022095, 'eval_accuracy': 0.744973544973545, 'eval_mcc': 0.0, 'eval_runtime': 21.9997, 'eval_samples_per_second': 42.955, 'eval_steps_per_second': 0.364, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c90eaa8fe64d81a6fb7f948c289a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22185108065605164, 'eval_accuracy': 0.744973544973545, 'eval_mcc': 0.0, 'eval_runtime': 22.1836, 'eval_samples_per_second': 42.599, 'eval_steps_per_second': 0.361, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e222c3caf90a4416a697d814fa16300d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25726646184921265, 'eval_accuracy': 0.744973544973545, 'eval_mcc': 0.0, 'eval_runtime': 22.1499, 'eval_samples_per_second': 42.664, 'eval_steps_per_second': 0.361, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d031d4aa6e5f45dabb96579b9b97de2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22308388352394104, 'eval_accuracy': 0.7439153439153439, 'eval_mcc': 0.21379903267059114, 'eval_runtime': 22.4142, 'eval_samples_per_second': 42.161, 'eval_steps_per_second': 0.357, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6b6b7c5e2e4b48b5d49f94e4e275bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2908986210823059, 'eval_accuracy': 0.762962962962963, 'eval_mcc': 0.22472282307392621, 'eval_runtime': 22.5869, 'eval_samples_per_second': 41.838, 'eval_steps_per_second': 0.354, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3522f09870c64195b3c58e3d9810a018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28922784328460693, 'eval_accuracy': 0.762962962962963, 'eval_mcc': 0.22472282307392621, 'eval_runtime': 22.2184, 'eval_samples_per_second': 42.532, 'eval_steps_per_second': 0.36, 'epoch': 9.0}\n",
      "{'loss': 0.4504, 'grad_norm': 0.44199422001838684, 'learning_rate': 0.0001, 'epoch': 9.09}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187b58aa85d24b1d8bbe07c61e61c610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2202126681804657, 'eval_accuracy': 0.7481481481481481, 'eval_mcc': 0.2587823105823372, 'eval_runtime': 22.1754, 'eval_samples_per_second': 42.615, 'eval_steps_per_second': 0.361, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 547.8976, 'train_samples_per_second': 126.374, 'train_steps_per_second': 1.004, 'train_loss': 0.42816899386319246, 'epoch': 10.0}\n",
      "ruT5_0.0001_0_128\n",
      "train {'train_runtime': 547.8976, 'train_samples_per_second': 126.374, 'train_steps_per_second': 1.004, 'total_flos': 2080047914680320.0, 'train_loss': 0.42816899386319246, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e76d48c3f74248b0be1c4cc22b6cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev {'validation_loss': 0.22064456343650818, 'validation_accuracy': 0.7481481481481481, 'validation_mcc': 0.2587823105823372, 'validation_runtime': 21.7138, 'validation_samples_per_second': 43.521, 'validation_steps_per_second': 0.368}\n"
     ]
    }
   ],
   "source": [
    "trainer = train(tokenizer, data_collator, tokenized_splits_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверим результат на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5ec202c1f1247d88a7db388472a8d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test {'test_loss': 0.20538544654846191, 'test_accuracy': 0.7721261444557477, 'test_mcc': 0.3281330304508942, 'test_runtime': 21.8893, 'test_samples_per_second': 44.908, 'test_steps_per_second': 0.365}\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset=tokenized_splits_data[\"test\"], max_length=10)\n",
    "print(\"test\", predictions.metrics)\n",
    "\n",
    "string_preds = tokenizer.batch_decode(predictions.predictions, skip_special_tokens=True)\n",
    "\n",
    "int_preds = [1 if prediction == POS_LABEL else 0 for prediction in string_preds]\n",
    "int_preds = np.asarray(int_preds)\n",
    "\n",
    "np.save(f\"results/preds.npy\", int_preds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
